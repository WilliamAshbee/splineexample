{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimplestToyProblem.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOFray4T70C0t/+X1xy+ko8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WilliamAshbee/splineexample/blob/main/SimplestToyProblem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "id": "lriVEWYxBqQJ",
        "outputId": "deb0e625-f7cc-4eff-ce2d-ba3acf703b3f"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pylab as plt\n",
        "from skimage import filters\n",
        "import math\n",
        "\n",
        "global numpoints\n",
        "numpoints = 1\n",
        "side = 32\n",
        "\n",
        "rows = torch.zeros(32,32)\n",
        "columns = torch.zeros(32,32)\n",
        "\n",
        "\n",
        "for i in range(32):\n",
        "    columns[:,i] = i\n",
        "    rows[i,:] = i\n",
        "\n",
        "\n",
        "def point_matrix():\n",
        "    length = side**2\n",
        "    canvas = torch.zeros((length,side, side))\n",
        "    \n",
        "\n",
        "    x = torch.zeros(length,numpoints) \n",
        "    y = torch.zeros(length,numpoints)\n",
        "    assert x.shape == (length,numpoints)\n",
        "    assert y.shape == (length,numpoints)\n",
        "    \n",
        "    points = torch.zeros(length,2)\n",
        "    for j in range(side):\n",
        "      for i in range(side):\n",
        "        l = i*32+j\n",
        "        canvas[l,i,j] = 1.0\n",
        "        x[l] = i\n",
        "        y[l] = j\n",
        "\n",
        "        #points[l,:,0] = x[l,:]\n",
        "        #points[l,:,1] = y[l,:]\n",
        "        points[l,0] = i#modified for lstm discriminator\n",
        "        points[l,1] = j#modified for lstm discriminator \n",
        "    \n",
        "    \n",
        "    return {\n",
        "        'canvas': canvas, \n",
        "        'points':points.type(torch.FloatTensor)}\n",
        "\n",
        "\n",
        "def plot_all( sample = None, model = None, labels = None):\n",
        "    img = sample[:,:].squeeze().cpu().numpy()\n",
        "    #img = np.zeros((32,32))\n",
        "    #img[1,1] = 1\n",
        "    #img[31,1] = 1\n",
        "    #img[31,31] = 1\n",
        "    img = img.T\n",
        "    plt.imshow(img, cmap=plt.cm.gray_r)\n",
        "    if model != None:\n",
        "        with torch.no_grad():\n",
        "            global numpoints\n",
        "\n",
        "            print(\"sample\", sample.shape)\n",
        "            sample = sample.cuda()\n",
        "            sample = sample.unsqueeze(0)\n",
        "            print('hello',sample.shape)\n",
        "            pred = model(sample)\n",
        "            print('hello')\n",
        "            \n",
        "            print('pred', pred.shape)\n",
        "            predres = 1\n",
        "            X = pred[:,0]\n",
        "            Y = pred[:,1]\n",
        "            \n",
        "            s = [10 for x in range(predres)]\n",
        "            \n",
        "            assert len(s) == predres\n",
        "            c = ['red' for x in range(predres)]\n",
        "            assert len(c) == predres\n",
        "            Y = Y.cuda()\n",
        "            X = X.cuda()\n",
        "            print(\"type\",type(X))\n",
        "            ascatter = plt.scatter(X.cpu().numpy(),Y.cpu().numpy(),s = s,c = c)\n",
        "            plt.gca().add_artist(ascatter)\n",
        "    else:\n",
        "        #print(labels.shape)\n",
        "\n",
        "        X = labels[0]\n",
        "        Y = labels[1]\n",
        "        #print(X.shape)\n",
        "        #print(Y.shape)\n",
        "        print(X,Y)\n",
        "        s = [.1 for x in range(numpoints)]\n",
        "        #print(len(s))\n",
        "        c = ['red' for x in range(numpoints)]\n",
        "        #print(len(c))\n",
        "        ascatter = plt.scatter(X,Y,s = s,c = c)\n",
        "        plt.gca().add_artist(ascatter)\n",
        "\n",
        "class PointDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Donut dataset.\"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.length = side**2\n",
        "        self.values = point_matrix()\n",
        "        assert self.values['canvas'].shape[0] == self.length\n",
        "        assert self.values['points'].shape[0] == self.length\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        canvas = self.values[\"canvas\"]\n",
        "        \n",
        "        canvas = canvas[idx,:,:]\n",
        "        assert canvas.shape == (side,side)\n",
        "        points = self.values[\"points\"]\n",
        "        points = points[idx,:]\n",
        "        return canvas, points\n",
        "    \n",
        "    @staticmethod\n",
        "    def displayCanvas(title,dataset, model):\n",
        "        #model.setBatchSize(batch_size = 1)\n",
        "        for i in range(36):\n",
        "            sample, labels = dataset[i]\n",
        "            plt.subplot(6,6,i+1)\n",
        "            plot_all(sample = sample,model=model, labels = labels)\n",
        "            plt.axis('off')\n",
        "        plt.savefig(title,dpi=600)\n",
        "\n",
        "dataset = PointDataset()\n",
        "PointDataset.displayCanvas('donut.png',dataset, model = None)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.) tensor(0.)\n",
            "tensor(0.) tensor(1.)\n",
            "tensor(0.) tensor(2.)\n",
            "tensor(0.) tensor(3.)\n",
            "tensor(0.) tensor(4.)\n",
            "tensor(0.) tensor(5.)\n",
            "tensor(0.) tensor(6.)\n",
            "tensor(0.) tensor(7.)\n",
            "tensor(0.) tensor(8.)\n",
            "tensor(0.) tensor(9.)\n",
            "tensor(0.) tensor(10.)\n",
            "tensor(0.) tensor(11.)\n",
            "tensor(0.) tensor(12.)\n",
            "tensor(0.) tensor(13.)\n",
            "tensor(0.) tensor(14.)\n",
            "tensor(0.) tensor(15.)\n",
            "tensor(0.) tensor(16.)\n",
            "tensor(0.) tensor(17.)\n",
            "tensor(0.) tensor(18.)\n",
            "tensor(0.) tensor(19.)\n",
            "tensor(0.) tensor(20.)\n",
            "tensor(0.) tensor(21.)\n",
            "tensor(0.) tensor(22.)\n",
            "tensor(0.) tensor(23.)\n",
            "tensor(0.) tensor(24.)\n",
            "tensor(0.) tensor(25.)\n",
            "tensor(0.) tensor(26.)\n",
            "tensor(0.) tensor(27.)\n",
            "tensor(0.) tensor(28.)\n",
            "tensor(0.) tensor(29.)\n",
            "tensor(0.) tensor(30.)\n",
            "tensor(0.) tensor(31.)\n",
            "tensor(1.) tensor(0.)\n",
            "tensor(1.) tensor(1.)\n",
            "tensor(1.) tensor(2.)\n",
            "tensor(1.) tensor(3.)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 36 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci_oKFKxBubp"
      },
      "source": [
        "d = dataset"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaEIxbzWHl2V",
        "outputId": "554d1815-e346-49ee-9a5d-9afc2e708026"
      },
      "source": [
        "l=2\n",
        "c = d[l][0]\n",
        "print(d[l][1][0],d[l][1][1])\n",
        "i= (int)(d[l][1][0].item())\n",
        "j=(int)(d[l][1][1].item())\n",
        "print(i,j)\n",
        "\n",
        "print(c[i,j])\n",
        "print(d[l][1])\n",
        "print(c)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.) tensor(2.)\n",
            "0 2\n",
            "tensor(1.)\n",
            "tensor([0., 2.])\n",
            "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7eBMvDeM-7_"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_dim = 1*32*32\n",
        "        self.hidden_dim = 2**11\n",
        "        self.midl=1\n",
        "        \n",
        "        self.inLayer = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, (int)(self.input_dim/2)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         )\n",
        "        self.midLayers1 = nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/2)+self.input_dim, (int)(self.input_dim/4)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )        \n",
        "        self.midLayers2 = nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/4)+self.input_dim, (int)(self.input_dim/8)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )        \n",
        "        self.midLayers3 = nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/8)+self.input_dim, (int)(self.input_dim/16)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )        \n",
        "        self.midLayers4 = nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/16)+self.input_dim, (int)(self.input_dim/32)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )        \n",
        "        self.out_layer = nn.Sequential(\n",
        "           nn.Linear((int)(self.input_dim/32)+self.input_dim, 2),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = x.squeeze()\n",
        "        x = torch.flatten(x,start_dim=1)\n",
        "        \n",
        "        #assert x.shape == (mini_batch,self.hidden_dim)\n",
        "        out = self.inLayer(x)\n",
        "        \n",
        "        out = torch.cat([out,x],dim = 1)\n",
        "        out = self.midLayers1(out)\n",
        "        \n",
        "        out = torch.cat([out,x],dim = 1)\n",
        "        out = self.midLayers2(out)\n",
        "        \n",
        "        out = torch.cat([out,x],dim = 1)\n",
        "        out = self.midLayers3(out)\n",
        "\n",
        "        out = torch.cat([out,x],dim = 1)\n",
        "        out = self.midLayers4(out)\n",
        "        \n",
        "        out = torch.cat([out,x],dim = 1)\n",
        "        out = self.out_layer(out)\n",
        "        \n",
        "        return 32.0*out\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxlGMipujWoc"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_dim = 1*32*32\n",
        "        self.hidden_dim = 2**11\n",
        "        self.midl=1\n",
        "        \n",
        "        self.inLayer = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, (int)(self.input_dim/2)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         )\n",
        "        \n",
        "        self.midLayers1 = nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/2), (int)(self.input_dim/4)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )\n",
        "\n",
        "        self.skip2 = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, (int)(self.input_dim/2)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear((int)(self.input_dim/2), (int)(self.input_dim/4)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         ) \n",
        "        self.midLayers2 = nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/4), (int)(self.input_dim/8)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )       \n",
        "\n",
        "        self.skip3 = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, (int)(self.input_dim/2)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear((int)(self.input_dim/2), (int)(self.input_dim/4)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear((int)(self.input_dim/4), (int)(self.input_dim/8)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         )\n",
        "        self.midLayers3 = nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/8), (int)(self.input_dim/16)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )\n",
        "\n",
        "        self.skip4 = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, (int)(self.input_dim/2)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear((int)(self.input_dim/2), (int)(self.input_dim/4)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear((int)(self.input_dim/4), (int)(self.input_dim/8)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "         \n",
        "            nn.Linear((int)(self.input_dim/8), (int)(self.input_dim/16)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         )\n",
        "        self.midLayers4 = nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/16), (int)(self.input_dim/32)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )   \n",
        "\n",
        "        self.skip5 = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, (int)(self.input_dim/2)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear((int)(self.input_dim/2), (int)(self.input_dim/4)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear((int)(self.input_dim/4), (int)(self.input_dim/8)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "         \n",
        "            nn.Linear((int)(self.input_dim/8), (int)(self.input_dim/16)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "         \n",
        "            nn.Linear((int)(self.input_dim/16), (int)(self.input_dim/32)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         )\n",
        "        \n",
        "        \n",
        "        self.midLayers5 = nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/32), (int)(self.input_dim/64)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )\n",
        "\n",
        "\n",
        "        self.skip6 = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, (int)(self.input_dim/2)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear((int)(self.input_dim/2), (int)(self.input_dim/4)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear((int)(self.input_dim/4), (int)(self.input_dim/8)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "         \n",
        "            nn.Linear((int)(self.input_dim/8), (int)(self.input_dim/16)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "         \n",
        "            nn.Linear((int)(self.input_dim/16), (int)(self.input_dim/32)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "         \n",
        "            nn.Linear((int)(self.input_dim/32), (int)(self.input_dim/64)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         )\n",
        "        \n",
        "\n",
        "\n",
        "        self.midLayers6 = nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/64), (int)(self.input_dim/128)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )   \n",
        "           \n",
        "        self.skipout = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, (int)(self.input_dim/128)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         )\n",
        "             \n",
        "        self.out_layer = nn.Sequential(\n",
        "           nn.Linear((int)(self.input_dim/128), 2),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = x.squeeze()\n",
        "        x = torch.flatten(x,start_dim=1)\n",
        "        \n",
        "        #assert x.shape == (mini_batch,self.hidden_dim)\n",
        "        out = self.inLayer(x)\n",
        "        \n",
        "        out = self.midLayers1(out)\n",
        "        \n",
        "        out += self.skip2(x)\n",
        "        out = self.midLayers2(out)\n",
        "        \n",
        "        out += self.skip3(x)\n",
        "        out = self.midLayers3(out)\n",
        "        afterMid3 = out\n",
        "\n",
        "        out+= self.skip4(x)\n",
        "        out = self.midLayers4(out)\n",
        "        \n",
        "        out+= self.skip5(x)\n",
        "        out = self.midLayers5(out)\n",
        "        \n",
        "        out+= self.skip6(x)\n",
        "        out = self.midLayers6(out)\n",
        "        \n",
        "        out += self.skipout(x)\n",
        "        out = self.out_layer(out)\n",
        "        \n",
        "        return 32.0*out\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeuqkTxZ1dJJ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_dim = 1*32*32\n",
        "        self.hidden_dim = 2**11\n",
        "        self.midl=1\n",
        "        \n",
        "        self.inLayer = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, (int)(self.input_dim/2)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         )\n",
        "        \n",
        "        self.midLayers1 = nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/2), (int)(self.input_dim/4)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )\n",
        "        self.midLayers2 = nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/4), (int)(self.input_dim/8)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )\n",
        "\n",
        "        \n",
        "        self.up1 = nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/8), (int)(self.input_dim/4)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )\n",
        "\n",
        "        self.up2 = nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/4), (int)(self.input_dim/2)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )\n",
        "        self.up3 = nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/2), (int)(self.input_dim)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )\n",
        "\n",
        "        self.down1 = nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim), (int)(self.input_dim/2)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )\n",
        "        self.down2 = nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/2), (int)(self.input_dim/4)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )\n",
        "        self.out_layer = nn.Sequential(\n",
        "          nn.Linear((int)(self.input_dim/4), 2),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.skip1 = nn.Sequential(\n",
        "         nn.Linear((int)(self.input_dim), (int)(self.input_dim)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )\n",
        "\n",
        "        self.skip2 = nn.Sequential(\n",
        "         nn.Linear((int)(self.input_dim/2), (int)(self.input_dim/2)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )\n",
        "\n",
        "        self.skip4 = nn.Sequential(\n",
        "         nn.Linear((int)(self.input_dim/4), (int)(self.input_dim/4)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )\n",
        "\n",
        "        self.skip8 = nn.Sequential(\n",
        "         nn.Linear((int)(self.input_dim/8), (int)(self.input_dim/8)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = x.squeeze()\n",
        "        x = torch.flatten(x,start_dim=1)\n",
        "        \n",
        "        #assert x.shape == (mini_batch,self.hidden_dim)\n",
        "        out = self.inLayer(x)\n",
        "        afterIn = out\n",
        "        out = self.midLayers1(out)\n",
        "        afterml1 = out\n",
        "        out = self.midLayers2(out)\n",
        "        afterml2 = out        \n",
        "        out = self.up1(out)\n",
        "        out+= self.skip4(afterml1)\n",
        "        out = self.up2(out)\n",
        "        out+= self.skip2(afterIn)\n",
        "        out = self.up3(out)\n",
        "        \n",
        "        out += self.skip1(x)\n",
        "        out = self.down1(out)\n",
        "         \n",
        "        out = self.down2(out)\n",
        "        \n",
        "        out = self.out_layer(out)\n",
        "        \n",
        "        return 32.0*out"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUs4KyjN90UH"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_dim = 1*32*32 #2**10\n",
        "        self.hidden_dim = 2**11\n",
        "        self.midl=1\n",
        "        assert self.input_dim ==2**10\n",
        "        self.downLayer = [nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/2**i), (int)(self.input_dim/2**(i+1))),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         ).cuda() for i in range(9)]\n",
        "        \n",
        "\n",
        "        self.upLayer = [nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/2**(i+1)), (int)(self.input_dim/2**(i))),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         ).cuda() for i in range(8,-1,-1)]\n",
        "        \n",
        "        self.finalDown = [nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/2**i), (int)(self.input_dim/2**(i+1))),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         ).cuda() for i in range(9)]\n",
        "\n",
        "        self.skip1 = [nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/2**i), (int)(self.input_dim/2**i)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         ).cuda() for i in range(9,-1,-1)]\n",
        "\n",
        "        self.skip2 = [nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/2**i), (int)(self.input_dim/2**i)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         ).cuda() for i in range(10)]\n",
        "\n",
        "        self.downLayer = nn.Sequential(*self.downLayer)\n",
        "        self.upLayer = nn.Sequential(*self.upLayer)\n",
        "        self.finalDown = nn.Sequential(*self.finalDown)\n",
        "        \n",
        "    def forward(self, x):\n",
        "      x = torch.flatten(x,start_dim=1)\n",
        "      out = x\n",
        "      p1 = [x]\n",
        "      for i in range(9):\n",
        "        out = self.downLayer[i](out)\n",
        "        p1.append(out)\n",
        "      \n",
        "      p1.reverse()\n",
        "      p2 = []\n",
        "      \n",
        "      for i in range(9):\n",
        "        out+=p1[i] \n",
        "        out = self.upLayer[i](out)\n",
        "        p2.append(out)\n",
        "      \n",
        "      p2.reverse()\n",
        "      for i in range(9):\n",
        "        out+=p2[i]\n",
        "        out = self.finalDown[i](out)\n",
        "        \n",
        "      #print('done', out.shape)\n",
        "      return out"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwDlypLIO6-9"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_dim = 1*32*32 #2**10\n",
        "        self.hidden_dim = 2**11\n",
        "        self.midl=1\n",
        "        assert self.input_dim ==2**10\n",
        "        self.downLayer = [nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/2**i), (int)(self.input_dim/2**(i+1))),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         ).cuda() for i in range(9)]\n",
        "        \n",
        "\n",
        "        self.upLayer = [nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/2**(i+1)), (int)(self.input_dim/2**(i))),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         ).cuda() for i in range(8,-1,-1)]\n",
        "        \n",
        "        self.finalDown = [nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/2**i), (int)(self.input_dim/2**(i+1))),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         ).cuda() for i in range(9)]\n",
        "\n",
        "        self.skip1 = [nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/2**i), (int)(self.input_dim/2**i)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         ).cuda() for i in range(9,-1,-1)]\n",
        "\n",
        "        self.skip2 = [nn.Sequential(\n",
        "            nn.Linear((int)(self.input_dim/2**i), (int)(self.input_dim/2**i)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout()\n",
        "         ).cuda() for i in range(10)]\n",
        "\n",
        "        self.downLayer = nn.Sequential(*self.downLayer)\n",
        "        self.upLayer = nn.Sequential(*self.upLayer)\n",
        "        self.finalDown = nn.Sequential(*self.finalDown)\n",
        "        \n",
        "    def forward(self, x):\n",
        "      x = torch.flatten(x,start_dim=1)\n",
        "      out = x\n",
        "      p1 = [x]\n",
        "      for i in range(9):\n",
        "        out = self.downLayer[i](out)\n",
        "        p1.append(out)\n",
        "      \n",
        "      p1.reverse()\n",
        "      p2 = []\n",
        "      \n",
        "      for i in range(9):\n",
        "        #out+=self.skip1[i](p1[i]) \n",
        "        out = self.upLayer[i](out)\n",
        "        p2.append(out)\n",
        "      p2.reverse()\n",
        "      \n",
        "      for i in range(9):\n",
        "        #print(i)\n",
        "        #out+=p2[i]\n",
        "        out = self.finalDown[i](out)\n",
        "        \n",
        "      #print('done', out.shape)\n",
        "      return out"
      ],
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYKNN1XTuMO1"
      },
      "source": [
        "model = MLP().cuda()"
      ],
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QreZ-k6dtTx"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr = 0.00001, betas = (.9,.999))"
      ],
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PfBoCUFuTkh",
        "outputId": "d86e9fe6-82ce-404a-d025-9060b8afbf31"
      },
      "source": [
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
        "\n",
        "mini_batch = 128\n",
        "dataset = PointDataset()\n",
        "loader_train = data.DataLoader(\n",
        "    dataset, \n",
        "    batch_size=mini_batch,\n",
        "    sampler=RandomSampler(data_source=dataset),\n",
        "    num_workers=4)\n"
      ],
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "KJz942ZhumMq",
        "outputId": "7b628c80-9449-4816-9f92-07bc6c67b0c7"
      },
      "source": [
        "epoch = 200\n",
        "for e in range(epoch):\n",
        "  for xin,yin in loader_train:\n",
        "    if xin.shape[0] != mini_batch:\n",
        "      print(xin.shape)\n",
        "      continue\n",
        "    xin = xin.cuda()\n",
        "    yin = yin.cuda()\n",
        "    out = model(xin)\n",
        "    loss = torch.mean((out-yin)**2)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "  print(e,'\\t', loss)\n"
      ],
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 \t tensor(312.5000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "1 \t tensor(318.1172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "2 \t tensor(358.8281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "3 \t tensor(329.0391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "4 \t tensor(328.2461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "5 \t tensor(322.8750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "6 \t tensor(297.1172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "7 \t tensor(313.6953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "8 \t tensor(278.1836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "9 \t tensor(308.7734, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "10 \t tensor(314.1875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "11 \t tensor(326.8867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "12 \t tensor(310.1250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "13 \t tensor(353.1328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "14 \t tensor(306.8516, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _releaseLock at 0x7fb821f228c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 221, in _releaseLock\n",
            "    def _releaseLock():\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEmpty\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-288-41b2375bd7fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mxin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myin\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 220740, 220742, 220744) exited unexpectedly"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CseZX1-9vKlP"
      },
      "source": [
        "dataset = PointDataset()\n",
        "PointDataset.displayCanvas('donut.png',dataset, model = model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n8xKdlYwjQd"
      },
      "source": [
        "a= torch.zeros(64,3)\n",
        "b = torch.zeros(64,4)\n",
        "c = torch.cat([a,b], dim = 1)\n",
        "print(c.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S8PURbSRrPD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}