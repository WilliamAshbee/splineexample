{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_mcfixed.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMe0uaqb5sHPPn1Z7RCzRyR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WilliamAshbee/splineexample/blob/main/LSTM_mcfixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GuTzOJ5E0oN"
      },
      "source": [
        "With progressive growing of the sequence, the loss drops more stably. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E5VVeE0nEn4"
      },
      "source": [
        "https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPJLPR-jCBtx",
        "outputId": "1454086c-6328-41c6-945d-df2136250653"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pylab as plt\n",
        "from skimage import filters\n",
        "import math\n",
        "\n",
        "global numpoints\n",
        "numpoints = 1000\n",
        "side = 32\n",
        "\n",
        "rows = torch.zeros(32,32)\n",
        "columns = torch.zeros(32,32)\n",
        "\n",
        "\n",
        "for i in range(32):\n",
        "    columns[:,i] = i\n",
        "    rows[i,:] = i\n",
        "\n",
        "\n",
        "def donut_matrix(length = 10):\n",
        "    radiusMax = side /3\n",
        "    w = 1\n",
        "    sigmas = [None, 1]\n",
        "    \n",
        "    canvas = torch.zeros((length,side, side))\n",
        "    r0 = torch.tensor(np.random.uniform(side/4, side/3, length))\n",
        "\n",
        "    radii = torch.zeros((length,numpoints))\n",
        "    radii[:, :] = r0.unsqueeze(1)\n",
        "    \n",
        "    ind = [x for x in range(numpoints)]\n",
        "\n",
        "    theta = torch.FloatTensor(ind)\n",
        "    theta *= math.pi*2.0/(float)(numpoints)\n",
        "    \n",
        "    for i in range(1,length):\n",
        "        a = np.random.uniform(1.0,3.0)*torch.sin(np.random.uniform(20.0)*theta+np.random.uniform(1000.0))\n",
        "        #a = 4.0*torch.sin(10.0*theta)\n",
        "        #print(a.shape,torch.max(a))\n",
        "        radii[i,:] += a\n",
        "        #print(radii.shape, torch.max(radii))\n",
        "    \n",
        "    assert torch.min(radii)>0\n",
        "    #print(radii.max(axis = 0)[0].shape)\n",
        "    rmaxs = radii.max(axis = 1)[0]\n",
        "    pmins = rmaxs+1.0\n",
        "    pmaxs = side-rmaxs-1.0\n",
        "    x0 = np.random.uniform(pmins,pmaxs)\n",
        "    y0 = np.random.uniform(pmins,pmaxs)\n",
        "    x0[:]=side/2\n",
        "    y0[:]=side/2\n",
        "    x0 = torch.tensor(x0)\n",
        "    y0 = torch.tensor(y0)\n",
        "    \n",
        "    x0 = x0.unsqueeze(1)\n",
        "    y0 = y0.unsqueeze(1)\n",
        "    #radii = torch.from_numpy(radii)\n",
        "    xrfactors = torch.cos(theta).unsqueeze(0)\n",
        "    yrfactors = torch.sin(theta).unsqueeze(0)\n",
        "    \n",
        "    print('x0_y0_r_xrf_yrf',x0.shape,y0.shape,radii.shape,xrfactors.shape,yrfactors.shape)\n",
        "\n",
        "    x = (x0+(xrfactors*radii))\n",
        "    y = (y0+(yrfactors*radii))\n",
        "    assert x.shape == (length,numpoints)\n",
        "    assert y.shape == (length,numpoints)\n",
        "    assert torch.sum(x[x>(side-1)])==0 \n",
        "    assert torch.sum(x[x<0])==0 \n",
        "    assert torch.sum(y[y>(side-1)])==0 \n",
        "    assert torch.sum(y[y<0])==0 \n",
        "    \n",
        "    points = torch.zeros(length,numpoints,2)\n",
        "    for l in range(length):\n",
        "        canvas[l,y[l,:].type(torch.LongTensor),x[l,:].type(torch.LongTensor)]=1.0\n",
        "        points[l,:,0] = x[l,:]#modified for lstm discriminator\n",
        "        points[l,:,1] = y[l,:]#modified for lstm discriminator \n",
        "    \n",
        "    \n",
        "    return {\n",
        "        'canvas': canvas, \n",
        "        'points':points.type(torch.FloatTensor)}\n",
        "\n",
        "def plot_all_model( sample = None, label=None):\n",
        "    X = label[:,0]\n",
        "    Y = label[:,1]\n",
        "    img = sample[:,:].squeeze().cpu().numpy()\n",
        "    plt.imshow(img, cmap=plt.cm.gray_r)\n",
        "    predres = 1000\n",
        "\n",
        "    s = [.001 for x in range(predres)]\n",
        "    \n",
        "    assert len(s) == predres\n",
        "    c = ['red' for x in range(predres)]\n",
        "    assert len(c) == predres\n",
        "    plt.scatter(X.cpu().numpy(),Y.cpu().numpy(),s = s,c = c)\n",
        "\n",
        "\n",
        "def getXYs(model=None,loader_disp = None):\n",
        "\n",
        "    outputs = torch.zeros(100,1000,2).cuda()\n",
        "    with torch.no_grad():\n",
        "      h0 = None\n",
        "      c0 = None\n",
        "      X = []\n",
        "      Y = []\n",
        "      for samples,labels in loader_disp:\n",
        "        for i in range(38):\n",
        "          optimizer.zero_grad()\n",
        "          if i == 0:\n",
        "            out, h0, c0,o,output = model(samples.cuda(),labels[:,0,:2].cuda(),h0,c0,ind = i)\n",
        "            outputs[:,i*25:(i+1)*25,:] = out.detach().clone()\n",
        "          else:\n",
        "            out, h0, c0,o,output = model(samples.cuda(),labels[:,0,:2].cuda(),h0,c0,ind = i,o = o,output=output)\n",
        "            outputs[:,i*25:(i+1)*25,:] = out.detach().clone()\n",
        "\n",
        "        return samples, outputs\n",
        "\n",
        "          #X.extend(out.detach()[0,:,0].tolist())\n",
        "          #Y.extend(out.detach()[0,:,1].tolist())\n",
        "\n",
        "          \n",
        "        \n",
        "\n",
        "def plot_all( sample = None, labels = None):\n",
        "    img = sample[:,:].squeeze().cpu().numpy()\n",
        "    plt.imshow(img, cmap=plt.cm.gray_r)\n",
        "    X = labels[:,0]\n",
        "    Y = labels[:,1]\n",
        "    s = [.001 for x in range(numpoints)]\n",
        "    c = ['red' for x in range(numpoints)]\n",
        "    plt.scatter(X.cpu().numpy(),Y.cpu().numpy(),s = s,c = c)\n",
        "    \n",
        "class DonutDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Donut dataset.\"\"\"\n",
        "    def __init__(self, length = 10):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.length = length\n",
        "        self.values = donut_matrix(length)\n",
        "        assert self.values['canvas'].shape[0] == self.length\n",
        "        assert self.values['points'].shape[0] == self.length\n",
        "        \n",
        "        count = 0\n",
        "        for i in range(self.length):\n",
        "          a = self[i]\n",
        "          c = a[0]\n",
        "          for el in a[1]:\n",
        "            #print(c[(int)(el[1]),(int)(el[0])].item())\n",
        "            #assert c[(int)(el[1]),(int)(el[0])].item() == 1\n",
        "            y,x = (int)(el[1]),(int)(el[0])\n",
        "            if x < side-2 and x > 2 and y < side-2 and y > 2: \n",
        "              if c[y,x] != 1 and \\\n",
        "                c[y+1,x] != 1 and c[y+1,-1+x] != 1 and c[y+1,1+x] != 1 and \\\n",
        "                c[y-1,x] != 1 and c[y,-1+x] != 1 and c[y,1+x] != 1:\n",
        "                count+=1\n",
        "        assert count ==0\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        canvas = self.values[\"canvas\"]\n",
        "        \n",
        "        canvas = canvas[idx,:,:]\n",
        "        assert canvas.shape == (side,side)\n",
        "        \n",
        "        points = self.values[\"points\"]\n",
        "        points = points[idx,:]\n",
        "        #points = points.unsqueeze(1)\n",
        "        #z = torch.zeros(numpoints,1)\n",
        "        #print(z.shape)\n",
        "        #points = torch.cat([points,z], dim = 1)\n",
        "        \n",
        "        #print('points', points.shape)\n",
        "        return canvas, points\n",
        "    \n",
        "    @staticmethod\n",
        "    def displayCanvas(title,dataset, model):\n",
        "        #model.setBatchSize(batch_size = 1)\n",
        "        if model != None:\n",
        "          model = model.eval()\n",
        "          loader_disp = data.DataLoader(\n",
        "            dataset, \n",
        "            batch_size=100,\n",
        "            num_workers=2)\n",
        "          samples, outputs = getXYs(model=model,loader_disp = loader_disp)\n",
        "          for i in range(100):\n",
        "            plt.subplot(10,10,i+1)\n",
        "            plot_all_model(samples[i,:,:],outputs[i,:,:])\n",
        "            plt.axis('off')\n",
        "\n",
        "        else:\n",
        "          for i in range(100):\n",
        "            sample, labels = dataset[i]\n",
        "            plt.subplot(10,10,i+1)\n",
        "            plot_all(sample = sample,labels = labels)\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.savefig(title,dpi=450)\n",
        "        plt.clf()\n",
        "        plt.cla()\n",
        "        plt.close()#should free memory\n",
        "        print('open file: ',title)\n",
        "dataset = DonutDataset(length = 100)\n",
        "\n",
        "DonutDataset.displayCanvas('donut.png',dataset, model = None)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x0_y0_r_xrf_yrf torch.Size([100, 1]) torch.Size([100, 1]) torch.Size([100, 1000]) torch.Size([1, 1000]) torch.Size([1, 1000])\n",
            "open file:  donut.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhiJIwh0PvXT",
        "outputId": "b2f6284a-685a-41e3-d9ae-2da11d9e1850"
      },
      "source": [
        "\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
        "\n",
        "mini_batch = 100\n",
        "dataset = DonutDataset(length = 100*20)\n",
        "loader_train = data.DataLoader(\n",
        "    dataset, \n",
        "    batch_size=mini_batch,\n",
        "    sampler=RandomSampler(data_source=dataset),\n",
        "    num_workers=2)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x0_y0_r_xrf_yrf torch.Size([2000, 1]) torch.Size([2000, 1]) torch.Size([2000, 1000]) torch.Size([1, 1000]) torch.Size([1, 1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHTtTB_x75NA"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.track = 0\n",
        "        # define the properties\n",
        "        self.embed_size = 2\n",
        "        self.hidden_size = 512\n",
        "        self.num_layers = 2\n",
        "        self.seq_len = 25\n",
        "        self.full_len = 1000\n",
        "        self.lm_len = int(self.full_len/self.seq_len)\n",
        "        self.mem_fac = 5\n",
        "        self.mem_index = 0\n",
        "\n",
        "        self.longtermMem_lstm = None\n",
        "        \n",
        "        self.lstm_cell = nn.LSTM(self.hidden_size, self.hidden_size,self.num_layers)\n",
        "\n",
        "        self.inject_image_lstm =  nn.Sequential(\n",
        "            nn.Linear(2+1024+6*self.hidden_size,self.hidden_size),\n",
        "            #nn.Dropout(),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Linear(self.hidden_size,2),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "\n",
        "        self.alpha_lstm = nn.Sequential(\n",
        "            nn.Linear(self.hidden_size,self.seq_len),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "        \n",
        "        self.alpha_long_lstm_r1 = nn.Sequential(\n",
        "            nn.Linear(self.hidden_size,int((self.seq_len/self.mem_fac)*self.lm_len)),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        "        self.alpha_long_lstm_r2 = nn.Sequential(\n",
        "            nn.Linear(self.hidden_size,int((self.seq_len/self.mem_fac)*self.lm_len)),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "        \n",
        "        self.alpha_long_lstm_r3 = nn.Sequential(\n",
        "            nn.Linear(self.hidden_size,int((self.seq_len/self.mem_fac)*self.lm_len)),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        "        self.alpha_long_lstm_r4 = nn.Sequential(\n",
        "            nn.Linear(self.hidden_size,int((self.seq_len/self.mem_fac)*self.lm_len)),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "        \n",
        "        self.embs = None #= torch.empty((self.batch_size, 1000, self.hidden_size)).cuda()\n",
        "\n",
        "\n",
        "    def forward(self, features,p0,h0 = None, c0= None, ind = 0, o = None,output = None):\n",
        "        assert features != None\n",
        "        assert len(features.shape) == 3\n",
        "        self.track +=1\n",
        "        batch_size = features.shape[0]\n",
        "        \n",
        "        p0 = torch.flatten(p0[:,:2],start_dim=1)\n",
        "        \n",
        "        if o == None:\n",
        "          out = p0\n",
        "          self.longtermMem_lstm = torch.zeros(batch_size,int((self.seq_len/self.mem_fac)*self.lm_len),self.hidden_size).cuda()\n",
        "          self.embs = torch.empty((batch_size, self.seq_len, self.hidden_size)).cuda()\n",
        "          self.mem_index = 0\n",
        "        else:\n",
        "          out = o\n",
        "        \n",
        "        if h0 == None and c0 == None:\n",
        "          h0 = torch.zeros(self.num_layers,batch_size,self.hidden_size).cuda()\n",
        "          c0 = torch.zeros(self.num_layers,batch_size,self.hidden_size).cuda()\n",
        "\n",
        "        hidden_state = h0\n",
        "        cell_state = c0\n",
        "        outputs = torch.empty((batch_size, self.seq_len, 2)).cuda()\n",
        "        \n",
        "        output = None\n",
        "\n",
        "        for t in range(self.seq_len):\n",
        "\n",
        "            \n",
        "          if output == None:\n",
        "            output = torch.zeros(batch_size,self.hidden_size).cuda()\n",
        "            outesread1_lstm = torch.zeros(batch_size,self.hidden_size).cuda()\n",
        "            outesreadl1_lstm = torch.zeros(batch_size,self.hidden_size).cuda()\n",
        "            outesreadl2_lstm = torch.zeros(batch_size,self.hidden_size).cuda()\n",
        "            outesreadl3_lstm = torch.zeros(batch_size,self.hidden_size).cuda()\n",
        "            outesreadl4_lstm = torch.zeros(batch_size,self.hidden_size).cuda()\n",
        "          else:\n",
        "            a1_lstm = self.alpha_lstm(output.view(batch_size,self.hidden_size))\n",
        "            outesreada1_lstm = self.embs.clone()*a1_lstm.unsqueeze(2)#clone\n",
        "            outesread1_lstm = torch.sum(outesreada1_lstm,dim=1)\n",
        "            \n",
        "            al1_lstm = self.alpha_long_lstm_r1(output.view(batch_size,self.hidden_size))\n",
        "            outesreada1_lstm = self.longtermMem_lstm.clone()*al1_lstm.unsqueeze(2)#clone\n",
        "            outesreadl1_lstm = torch.sum(outesreada1_lstm,dim=1)\n",
        "            \n",
        "            al2_lstm = self.alpha_long_lstm_r2(output.view(batch_size,self.hidden_size))\n",
        "            outesreada2_lstm = self.longtermMem_lstm.clone()*al2_lstm.unsqueeze(2)#clone\n",
        "            outesreadl2_lstm = torch.sum(outesreada2_lstm,dim=1)\n",
        "            \n",
        "            al3_lstm = self.alpha_long_lstm_r3(output.view(batch_size,self.hidden_size))\n",
        "            outesreada3_lstm = self.longtermMem_lstm.clone()*al3_lstm.unsqueeze(2)#clone\n",
        "            outesreadl3_lstm = torch.sum(outesreada3_lstm,dim=1)\n",
        "            \n",
        "            al4_lstm = self.alpha_long_lstm_r4(output.view(batch_size,self.hidden_size))\n",
        "            outesreada4_lstm = self.longtermMem_lstm.clone()*al4_lstm.unsqueeze(2)#clone\n",
        "            outesreadl4_lstm = torch.sum(outesreada4_lstm,dim=1)\n",
        "            \n",
        "\n",
        "            \n",
        "          combin_lstm = torch.cat([torch.flatten(features,start_dim=1),out,output.view(batch_size,self.hidden_size),outesread1_lstm,outesreadl1_lstm,outesreadl2_lstm,outesreadl3_lstm,outesreadl4_lstm],dim=1)\n",
        "          combin_lstm = self.inject_image_lstm(combin_lstm)\n",
        "          \n",
        "          self.embs[:, t, :] = output.view(batch_size,self.hidden_size).clone()#.detach()\n",
        "          \n",
        "          output, (hidden_state, cell_state) = self.lstm_cell(combin_lstm.unsqueeze(0), (hidden_state, cell_state))\n",
        "          if t%self.mem_fac == 0:\n",
        "            self.longtermMem_lstm[:,self.mem_index,:] = output.view(batch_size,self.hidden_size).detach()\n",
        "            self.mem_index+=1\n",
        "          \n",
        "\n",
        "          out = self.fc_out(output.view(batch_size,self.hidden_size))\n",
        "          out = out*32.0\n",
        "          outputs[:, t, :] = out\n",
        "            \n",
        "        return outputs, hidden_state.detach(), cell_state.detach(), out.detach(),output.detach()#, captions\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50cqsTMxQXEm"
      },
      "source": [
        "model = DecoderRNN().cuda()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32G-MJC_QcKP"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr = 0.0001, betas = (.9,.999))#ideal\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60xL8p9QeQc-"
      },
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjpFwX8MQlPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "022e000e-2ce5-423f-8587-244e8e87b502"
      },
      "source": [
        "model = model.train()\n",
        "for seql in range(3,38,4):\n",
        "  #seql = 19\n",
        "  for epoch in range (4):\n",
        "    loss = None\n",
        "    for x,y in loader_train:\n",
        "      h0 = None\n",
        "      c0 = None\n",
        "      for i in range(seql):\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        if i == 0:\n",
        "          out, h0, c0,o,output = model(x,y[:,0,:2],h0,c0,ind = i)\n",
        "        else:\n",
        "          out, h0, c0,o,output = model(x,y[:,0,:2],h0,c0,ind = i,o = o,output=output)\n",
        "        loss = torch.mean((out-y[:,1+(25*i):1+(25*(i+1)),:2])**2)\n",
        "        #writer.add_scalar(\"Loss/train\", loss, epoch)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      #print(y[:,0,:2])\n",
        "      print('epoch',epoch,'seql',seql)\n",
        "      print(loss)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a\n",
            "b\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0 seql 1\n",
            "tensor(35.7087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(30.5459, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(28.1397, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(22.6423, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(19.9387, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(15.6052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(11.1572, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(7.5264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(5.3828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(4.8236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(4.9272, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(6.2538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(6.8162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(6.4392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(6.2519, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(5.3195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(4.8787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(3.9004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(4.0123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 1\n",
            "tensor(3.6727, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "b\n",
            "epoch 1 seql 1\n",
            "tensor(3.2054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(3.7699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(3.0474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(3.9161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(3.2580, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(3.2395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(3.8822, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(3.3425, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(2.8659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(3.1399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(3.0844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(2.8419, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(2.4176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(2.6610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(2.7038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(2.4336, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(2.7355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(2.6183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(2.5442, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 1\n",
            "tensor(2.5255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "b\n",
            "epoch 2 seql 1\n",
            "tensor(2.3628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(2.5631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(2.6724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(2.5442, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(2.3104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(2.2896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(2.4971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(2.0298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(2.2506, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(2.1663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(2.2433, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(2.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(2.1774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(2.1744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(2.0717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(2.0935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(1.9797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(2.0678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(1.9808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 1\n",
            "tensor(2.0769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "b\n",
            "epoch 3 seql 1\n",
            "tensor(2.0604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.8986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.7822, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(2.0407, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.7549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.6560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.6875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.7939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.9516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.6460, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.6656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.4717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.7460, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.6088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.5180, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.6884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.4741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.4791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.6338, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 1\n",
            "tensor(1.6162, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWZWJn4i_R5x"
      },
      "source": [
        "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.0001, betas = (.9,.999))#ideal\n",
        "\n",
        "for g in optimizer.param_groups:\n",
        "    g['lr'] = 0.0001\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nV5RNgqjYMgZ",
        "outputId": "577b7164-36fc-4b4c-b2e9-d5c51ed99737"
      },
      "source": [
        "\n",
        "model = model.train()\n",
        "#for seql in range(15,20,4):\n",
        "seql = 38\n",
        "for epoch in range (15):\n",
        "  loss = None\n",
        "  for x,y in loader_train:\n",
        "    h0 = None\n",
        "    c0 = None\n",
        "    for i in range(seql):\n",
        "      x = x.cuda()\n",
        "      y = y.cuda()\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      if i == 0:\n",
        "        out, h0, c0,o,output = model(x,y[:,0,:2],h0,c0,ind = i)\n",
        "      else:\n",
        "        out, h0, c0,o,output = model(x,y[:,0,:2],h0,c0,ind = i,o = o,output=output)\n",
        "      loss = torch.mean((out-y[:,1+(25*i):1+(25*(i+1)),:2])**2)\n",
        "      writer.add_scalar(\"Loss/train\", loss, epoch)\n",
        "      \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    #print(y[:,0,:2])\n",
        "    print('epoch',epoch,'seql',seql)\n",
        "    print(loss)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0 seql 38\n",
            "tensor(6.8810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(6.2873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(3.7074, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(2.1764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(5.0846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(2.1647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(4.2134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(12.1028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(3.5335, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(3.3034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(3.3715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(7.3361, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(2.0153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(2.0995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(2.0426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.4718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.9948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.5945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.4608, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.5089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.4282, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.2460, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.1783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.2891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.0996, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.2476, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.3792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(2.3690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.2578, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(5.9371, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.7808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.5924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(2.5956, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.4526, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(4.4745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.8716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.4803, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(2.1776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.3349, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(2.0542, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.3070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.4095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.4751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.2860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.5757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.1652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.1043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.7237, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.2422, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.7576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.3452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.8001, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6b5e30bede38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss/train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_scalar\u001b[0;34m(self, tag, scalar_value, global_step, walltime)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mscalar_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFetchBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         self._get_file_writer().add_summary(\n\u001b[0;32m--> 345\u001b[0;31m             scalar(tag, scalar_value), global_step, walltime)\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_scalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_scalar_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalltime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/tensorboard/summary.py\u001b[0m in \u001b[0;36mscalar\u001b[0;34m(name, scalar, collections)\u001b[0m\n\u001b[1;32m    245\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mwrong\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \"\"\"\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0mscalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scalar should be 0D'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mscalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/tensorboard/_convert_np.py\u001b[0m in \u001b[0;36mmake_np\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_prepare_pytorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     raise NotImplementedError(\n\u001b[1;32m     25\u001b[0m         'Got {}, but numpy array, torch tensor, or caffe2 blob name are expected.'.format(type(x)))\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/tensorboard/_convert_np.py\u001b[0m in \u001b[0;36m_prepare_pytorch\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuGEdcCjYLK5"
      },
      "source": [
        "#optimizer = torch.optim.Adam(model.parameters(),lr = 0.00001, betas = (.9,.999))#ideal\n",
        "for g in optimizer.param_groups:\n",
        "    g['lr'] = 0.00001\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXg-Ue7XkTAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a75590-b147-4ea3-a715-b2f75870c890"
      },
      "source": [
        "\n",
        "model = model.train()\n",
        "#for seql in range(15,20,4):\n",
        "seql = 38\n",
        "for epoch in range (40):\n",
        "  loss = None\n",
        "  for x,y in loader_train:\n",
        "    h0 = None\n",
        "    c0 = None\n",
        "    for i in range(seql):\n",
        "      x = x.cuda()\n",
        "      y = y.cuda()\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      if i == 0:\n",
        "        out, h0, c0,o,output = model(x,y[:,0,:2],h0,c0,ind = i)\n",
        "      else:\n",
        "        out, h0, c0,o,output = model(x,y[:,0,:2],h0,c0,ind = i,o = o,output=output)\n",
        "      loss = torch.mean((out-y[:,1+(25*i):1+(25*(i+1)),:2])**2)\n",
        "      writer.add_scalar(\"Loss/train\", loss, epoch)\n",
        "      \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    #print(y[:,0,:2])\n",
        "    print('epoch',epoch,'seql',seql)\n",
        "    print(loss)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0 seql 38\n",
            "tensor(1.5352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.3936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.3485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.2763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.3192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.1847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.2072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.2242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.3319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.2832, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.1752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.3028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.2268, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.1796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.1668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.2632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.0737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.0288, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.2566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 0 seql 38\n",
            "tensor(1.3613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.1412, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.2313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.1946, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.2996, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.2724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.2882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.1573, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.1784, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.1357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.2789, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.2341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.1625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.2052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.1717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.0752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.1023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.2193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.1837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 1 seql 38\n",
            "tensor(1.1074, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.0897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.1709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.1778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.0840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.1080, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.2110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.2000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.0609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.1649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.1166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.1841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.0737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.1155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.1362, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.0662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.2899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.1715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.2065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.1739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 2 seql 38\n",
            "tensor(1.2981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.1183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.1062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.1308, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.1482, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.1357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.2002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.2064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.0185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.0804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.2270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.1876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.0884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.1369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.3092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.1069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.0652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.1031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.1699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.0535, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 3 seql 38\n",
            "tensor(1.0684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.0701, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.1898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.1446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.0370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.0832, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.1608, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.1487, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.0750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.0981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.1060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(0.9775, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.1739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.1847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.0576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.0223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.0930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.2394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.0617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.2580, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 4 seql 38\n",
            "tensor(1.0414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.2454, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(0.8674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.1358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.0704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.0370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.1175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.2573, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.0818, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.1235, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.1744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.1327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(0.9531, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.0816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.1908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.0083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.1000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.2081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.1122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 5 seql 38\n",
            "tensor(1.0300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(1.0590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(1.0464, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(0.9851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(1.1499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(1.2843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(1.0897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(1.0877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(1.2591, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(1.2033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(1.2200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(0.9271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(1.1445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(1.0142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(1.0369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(1.0195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(1.0820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(1.1396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(1.1286, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(1.0516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 6 seql 38\n",
            "tensor(0.9585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.2576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.0486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.0121, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.0928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(0.9644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.0718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.0342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.0636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.2331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.0078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.0973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.1653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.0496, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.1126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.0731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.1821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.1264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(1.0275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 7 seql 38\n",
            "tensor(0.9853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(1.0112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(0.9268, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(1.0095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(1.0166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(1.0932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(1.0668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(0.9764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(1.2078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(1.0690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(1.0032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(1.2258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(0.9943, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(1.1519, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(1.1745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(1.1000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(0.9512, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(0.9938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(0.9287, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(1.0786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 8 seql 38\n",
            "tensor(1.0989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(1.1696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(1.1717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(1.4265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(1.0999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(1.1226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(1.1256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(1.1155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(1.1225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(0.9923, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(1.0572, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(1.1229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(0.9290, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(0.9624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(1.0287, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(1.0331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(0.9697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(1.0250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(1.0218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(1.0708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "epoch 9 seql 38\n",
            "tensor(1.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EorbMkSA9DmY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cf59103b-776e-4020-e385-11da4024a5fa"
      },
      "source": [
        "model = model.eval()\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
        "\n",
        "with torch.no_grad():\n",
        "  for epoch in range (3):\n",
        "\n",
        "    mini_batch = 100\n",
        "    test_dataset = DonutDataset(length = 100*10)\n",
        "    loader_test = data.DataLoader(\n",
        "        test_dataset, \n",
        "        batch_size=mini_batch,\n",
        "        sampler=RandomSampler(data_source=test_dataset),\n",
        "        num_workers=2)\n",
        "\n",
        "    loss = None\n",
        "    for x,y in loader_test:\n",
        "      h0 = None\n",
        "      c0 = None\n",
        "      seql = 38\n",
        "      loss = torch.zeros(seql)\n",
        "      for i in range(seql):\n",
        "        optimizer.zero_grad()\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "        \n",
        "        #print(x.shape,y.shape)\n",
        "        if i == 0:\n",
        "          out, h0, c0,o,output = model(x,y[:,0,:2],h0,c0,ind = i)\n",
        "        else:\n",
        "          out, h0, c0,o,output = model(x,y[:,0,:2],h0,c0,ind = i,o = o,output=output)\n",
        "          \n",
        "        loss[i] = torch.mean((out-y[:,1+(25*i):1+(25*(i+1)),:2])**2)\n",
        "        print(loss[i])\n",
        "        writer.add_scalar(\"Loss/val\", loss[i],i, epoch)\n",
        "        \n",
        "\n",
        "      #print(y[:,0,:2])\n",
        "      print('epoch',epoch)\n",
        "      print('test loss', torch.mean(loss))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x0_y0_r_xrf_yrf torch.Size([1000, 1]) torch.Size([1000, 1]) torch.Size([1000, 1000]) torch.Size([1, 1000]) torch.Size([1, 1000])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "epoch 0\n",
            "test loss tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "epoch 0\n",
            "test loss tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "epoch 0\n",
            "test loss tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "epoch 0\n",
            "test loss tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "epoch 0\n",
            "test loss tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "epoch 0\n",
            "test loss tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "epoch 0\n",
            "test loss tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "epoch 0\n",
            "test loss tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "epoch 0\n",
            "test loss tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "tensor(nan)\n",
            "epoch 0\n",
            "test loss tensor(nan)\n",
            "x0_y0_r_xrf_yrf torch.Size([1000, 1]) torch.Size([1000, 1]) torch.Size([1000, 1000]) torch.Size([1, 1000]) torch.Size([1, 1000])\n",
            "tensor(86.8330)\n",
            "tensor(76.8571)\n",
            "tensor(6.8802)\n",
            "tensor(2.0284)\n",
            "tensor(1.9227)\n",
            "tensor(1.6418)\n",
            "tensor(1.0576)\n",
            "tensor(1.2474)\n",
            "tensor(1.2451)\n",
            "tensor(1.2447)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c9fc9b3985aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m           \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m           \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-85c6228e24fc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, p0, h0, c0, ind, o, output)\u001b[0m\n\u001b[1;32m    125\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m           \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombin_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem_fac\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlongtermMem_lstm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 662\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    663\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AZseYD-Qotc"
      },
      "source": [
        "writer.flush()\n",
        "writer.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9qtXdY1gAgc"
      },
      "source": [
        "#%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7IqU3hQfmMa"
      },
      "source": [
        "#!kill 3285\n",
        "#%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S6n0-Q4xAg5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f9e9f4-4dfc-40af-a48d-228ed5c82469"
      },
      "source": [
        "pred_dataset = DonutDataset(length = 100)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x0_y0_r_xrf_yrf torch.Size([100, 1]) torch.Size([100, 1]) torch.Size([100, 1000]) torch.Size([1, 1000]) torch.Size([1, 1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgr2Hthdsoak"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8vn01RqKGky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "383c2a6e-f86d-46c6-ea7f-08028742dc0c"
      },
      "source": [
        "with torch.no_grad():\n",
        "  model = model.eval()\n",
        "  DonutDataset.displayCanvas('lstmpredictions.png',pred_dataset, model = model)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "open file:  lstmpredictions.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU7i4ppqFBLg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e6f7a4f-584c-4c80-deaf-64c52ce3b780"
      },
      "source": [
        "with torch.no_grad():\n",
        "  model = model.eval()\n",
        "  DonutDataset.displayCanvas('trainlstmpredictions.png',dataset, model = model)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "open file:  trainlstmpredictions.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pki9u_MRGXEP"
      },
      "source": [
        "model = model.eval()\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
        "\n",
        "with torch.no_grad():\n",
        "  for epoch in range (3):\n",
        "\n",
        "    mini_batch = 100\n",
        "    test_dataset = DonutDataset(length = 100*10)\n",
        "    loader_test = data.DataLoader(\n",
        "        test_dataset, \n",
        "        batch_size=mini_batch,\n",
        "        sampler=RandomSampler(data_source=test_dataset),\n",
        "        num_workers=2)\n",
        "\n",
        "    loss = None\n",
        "    for x,y in loader_test:\n",
        "      h0 = None\n",
        "      c0 = None\n",
        "      seql = 38\n",
        "      loss = torch.zeros(seql)\n",
        "      for i in range(seql):\n",
        "        optimizer.zero_grad()\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "        \n",
        "        #print(x.shape,y.shape)\n",
        "        if i == 0:\n",
        "          out, h0, c0,o,output = model(x,y[:,0,:2],h0,c0,ind = i)\n",
        "        else:\n",
        "          out, h0, c0,o,output = model(x,y[:,0,:2],h0,c0,ind = i,o = o,output=output)\n",
        "          \n",
        "        loss[i] = torch.mean((out-y[:,1+(25*i):1+(25*(i+1)),:2])**2)\n",
        "        print(loss[i])\n",
        "        writer.add_scalar(\"Loss/val\", loss[i],i, epoch)\n",
        "        \n",
        "\n",
        "      #print(y[:,0,:2])\n",
        "      print('epoch',epoch)\n",
        "      print('test loss', torch.mean(loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j28U5Ae-LlZH"
      },
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWSvK_N0GvaF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}